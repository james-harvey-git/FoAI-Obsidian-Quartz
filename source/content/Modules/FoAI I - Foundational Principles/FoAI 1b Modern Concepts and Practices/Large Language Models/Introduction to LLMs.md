---
type: lecture
module: FoAI I - Foundational Principles
submodule: FoAI 1b Modern Concepts and Practices
topic: Large Language Models
module_tag: foai-i-foundational-principles
submodule_tag: foai-1b-modern-concepts-and-practices
topic_tag: large-language-models
date: 2025-11-05
tags:
  - lecture
  - foai-i-foundational-principles
  - foai-1b-modern-concepts-and-practices
  - large-language-models
---

# Introduction to LLMs

![[../../../../Resources/PDFs/Introduction to LLMs.pdf]]
![[../../../../Resources/PDFs/ttlm_support.pdf]]
![[../../../../Resources/PDFs/Introduction to LLMs Notes.pdf]]

## ğŸ§­ Overview
This lecture introduces **Large Language Models (LLMs)** as next-token predictors that model language through **autoregressive probability factorization**. It traces the shift from **Shannonâ€™s memorization-based models** to **neural approaches** that generalize via **compression and pattern learning**. Key topics include **tokenization with Byte Pair Encoding (BPE)**, the **Transformer architecture** (embeddings, attention, residuals), and **text generation methods** such as temperature and top-k/p sampling. It concludes with **post-training alignment**â€”notably **RLHF**, framed as **Bayesian inference** aligning model behavior with human preferences.

## ğŸ§© Key Concepts
- [[Language Modeling]]
- [[Concept-2]]

## ğŸ§® Core Equations
```math
% Add derivations here
```

## ğŸ’¡ Insights
>[!tip]
>Language models are just next-word (token) predictors.

## â“Questions/Gaps

## ğŸ”— Connections
- [[Related Concept]]
- 